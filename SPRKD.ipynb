{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SPRKD (SADDLE POINT RECRUITMENT FOR KNOWLEDGE DISTILLATION): HIGHER-ORDER EXPERIMENTATION AND ANALYSIS.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install necesssary libraries\n",
    "#Hessian eigenthings and PyHessian\n",
    "!pip install --upgrade \"git+https://github.com/thetechdude124/pytorch-hessian-eigenthings.git@master#egg=hessian-eigenthings\"\n",
    "#Install PyHessian library (personal fork with version-specific updates)\n",
    "!pip install --upgrade \"git+https://github.com/thetechdude124/pyhessian.git@master#egg=pyhessian\"\n",
    "#PIL for image processing\n",
    "!pip install pillow-simd\n",
    "#Change numpy to version 1.24.0 (needed for TLI saddle injection)\n",
    "!pip uninstall numpy\n",
    "!pip install numpy==1.24.0\n",
    "#For storage and other dependencies\n",
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from pyhessian import hessian\n",
    "import numpy as np\n",
    "from hessian_eigenthings import compute_hessian_eigenthings\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import fastai\n",
    "from fastai.vision.all import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#Set GPU device\n",
    "torch.cuda.set_device(0)\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPRKD class\n",
    "#Import libraries\n",
    "import math\n",
    "#Memory optimizer class\n",
    "class SPRKD(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    IMPLEMENTATION OF SPRKD - SADDLE POINT RECRUITMENT FOR KNOWLEDGE DISTILLATION.\n",
    "    \n",
    "    \"\"\"\n",
    "    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\n",
    "    #Initialize optimizer object\n",
    "    def __init__(self, params, loss_function, stepsize = 0.001, bias = 0.001, generosity = 5, exploration_steps = 500, \n",
    "                is_teacher = False, teacher_saddle_points = [], optimizer = None, epsilon = 10e-3, PGD_delta = 5, \n",
    "                max_hessian_neg_eigensteps = 50, PGD_epoch_limit = 100, cooldown_steps = 20, decay = 100, selfKD = True, stride = 1):\n",
    "        #Check whether parameters are within bounds\n",
    "        if stepsize <= 0:\n",
    "            raise ValueError(\"Invalid stepsize '{}' provided. 'stepsize' must be in the range (0, inf).\")\n",
    "        if (generosity <= 0 or generosity > 10) or isinstance(generosity, float):\n",
    "            raise ValueError(\"Invalid generosity score provided. 'generosity' must be an integer in the range [1, 10].\")\n",
    "        if exploration_steps <= 0:\n",
    "            raise ValueError(\"Invalid # of steps provided for exploration phase. Must be >= 0.\")\n",
    "        if type(is_teacher) is not bool:\n",
    "            raise ValueError(\"Invalid value for 'is_teacher' provided, expected True or False.\")\n",
    "        #Declare DEFAULTS with provided values\n",
    "        DEFAULTS = dict(loss_function = loss_function, stepsize = stepsize, bias = bias, generosity = generosity, exploration_steps = exploration_steps, \n",
    "                        is_teacher = is_teacher, teacher_saddle_points = teacher_saddle_points, optim_function = optimizer, epsilon = epsilon, PGD_delta = PGD_delta,\n",
    "                        max_hessian_neg_eigensteps = max_hessian_neg_eigensteps, PGD_epoch_limit = PGD_epoch_limit, cooldown_steps = cooldown_steps, decay = decay, \n",
    "                        selfKD = selfKD, stride = stride)\n",
    "        #Initialize optimizer\n",
    "        super(SPRKD, self).__init__(params, DEFAULTS)\n",
    "\n",
    "    #Method for optimization step\n",
    "    def step(self, model, current_loss, n_eigs = 2, closure = None):\n",
    "        #Set loss to None\n",
    "        return_loss = None\n",
    "        #Loss must become closure function if it is defined, else remain the same\n",
    "        return_loss = closure() if closure != None else return_loss\n",
    "        #Check if we are on the first iteration of the optimizer - if so, setup variables \n",
    "        if not self.state[\"STEP\"]:\n",
    "                #Set step to 1\n",
    "                self.state[\"STEP\"] = 1\n",
    "                #Create list to hold saddle points and model parameters at those points\n",
    "                self.state[\"SADDLE_POINT_PARAMS\"] = []\n",
    "                self.state[\"PHASE\"] = \"EXPLORATORY\"\n",
    "                self.state[\"GRADIENTS\"] = {}\n",
    "                #Define cooldown period for saddle point reversions\n",
    "                self.state[\"COOLDOWN_STEPS\"] = self.param_groups[0][\"cooldown_steps\"]\n",
    "                #Create dictionary of booleans to track whether each parameter matrix should be allowed to converge to the teacher saddle point region \n",
    "                self.state[\"ALLOW_TEACHER_SADDLE_STEPS\"] = {}\n",
    "                #Create counter to track the number of Hessian negative eigensteps taken\n",
    "                self.state[\"N_HESSIAN_NEG_EIGENSTEPS\"] = 0\n",
    "                #Create a norm threshold for the given gradients - minimum value before which gradients likely reflect saddle point plateaus\n",
    "                self.state[\"GRADIENT_THRESHOLD\"] = torch.tensor(0.01)\n",
    "                #Add history of parameter values for Peterubed Gradient Descent Mechanics\n",
    "                self.state[\"PARAM_HISTORY_PGD\"] = {}\n",
    "                #Save loss for same mechanics (needed to evaluate whether the Perterbation was successful or if reversion is necessary)\n",
    "                self.state[\"STORED_LOSS\"] = 0.0\n",
    "        #If we are not on the first iteration, increment the current step and set loss \n",
    "        else: \n",
    "            self.state[\"STEP\"] += 1\n",
    "        for param_group in self.param_groups:\n",
    "            #heck to see if any parameters are outside the epsillon delta range of the saddle point region\n",
    "            #Do so only if the current model is not a teacher\n",
    "            if param_group[\"is_teacher\"]: param_group[\"optim_function\"].step()\n",
    "            #The model is otherwise a student - take a step if approximated saddle region reached\n",
    "            elif True in self.state[\"ALLOW_TEACHER_SADDLE_STEPS\"].values(): print(\"\\nDisabled. No ADAM step.\")\n",
    "                # print(self.state[\"ALLOW_TEACHER_SADDLE_STEPS\"])\n",
    "            #If True is not in the dictionary, it must be set to false -> take a step\n",
    "            else: param_group[\"optim_function\"].step(), print(self.state[\"ALLOW_TEACHER_SADDLE_STEPS\"])\n",
    "            #Check if the model being optimized is a student or teacher\n",
    "            #If teacher, detect saddle points via Hessian eigenvalues approximation and density\n",
    "            if param_group[\"is_teacher\"]: self.determineSaddlePoint(model = model, n_eigs = n_eigs, param_group = param_group, step_delta = 100)\n",
    "            #If student, apply Transformation Matrix, Negative Hessian Eigensteps, and/or PGD steps\n",
    "            elif not param_group[\"is_teacher\"]:\n",
    "                #Determine if TM application is needed\n",
    "                self.applyTransformationMatrix(param_group = param_group)\n",
    "                #Conditionally apply Negative Hessian Eigensteps\n",
    "                self.negativeHessianEigensteps(model = model, n_eigs = n_eigs, param_group = param_group)\n",
    "                #Determine PGD necessity and apply accordingly\n",
    "                self.perturbedGD(model = model, current_loss = current_loss, param_group = param_group)\n",
    "                #Decrement cooldown steps if not already zero\n",
    "                if self.state[\"COOLDOWN_STEPS\"] > 0: self.state[\"COOLDOWN_STEPS\"] -= 1\n",
    "        return return_loss\n",
    "\n",
    "    #Determine if a teacher model is at a saddle point via Hessian eigenvalue directional density\n",
    "    def determineSaddlePoint(self, model, n_eigs, param_group, step_delta = 100):\n",
    "        #Approximate the eigenvalues of the Hessian matrix and find determinant + trace.\n",
    "        #Create Hessian computation object\n",
    "        hess_comp = hessian(model = model.model, criterion = model.loss_func, data = next(iter(model.dls.train)), cuda = True)\n",
    "        #Find top two eigenvalues/eigenvectors\n",
    "        top_eigenvalues, top_eigenvectors = hess_comp.eigenvalues(top_n = n_eigs)\n",
    "        #Get # of positive and negative eigenvalues\n",
    "        pos_eigs, neg_eigs, zero_eigs = [], [], []\n",
    "        for eigenvalue in top_eigenvalues: \n",
    "            if eigenvalue > 0: pos_eigs.append(eigenvalue)\n",
    "            elif eigenvalue == 0: zero_eigs.append(eigenvalue) \n",
    "            else: neg_eigs.append(eigenvalue)\n",
    "        #Compare total magnitude of positive and negative directions - if the magnitude of negative directions is at least 40% of positive direction magntidue, append saddle point\n",
    "        print(\"\\nNEG EIG SUM:\", abs(sum(neg_eigs)))\n",
    "        print(\"POS EIGE SUM:\", sum(pos_eigs))\n",
    "        print(\"TOP EIGENVALUES:\", top_eigenvalues)\n",
    "        if abs(sum(neg_eigs)) >= (0.4 * sum(pos_eigs)):\n",
    "            #Clone and detach parameters before appending (otherwise, all parameter matrices become linked together and thereby become identical)\n",
    "            #Iterate over each parameter, detach, and store in a new list object\n",
    "            detached_saddle_point = [parameter.clone().detach().to('cpu') for parameter in param_group[\"params\"]]\n",
    "            self.state[\"SADDLE_POINT_PARAMS\"].append(detached_saddle_point), print('APPENDED SADDLE POINTS.', end = \"\")\n",
    "\n",
    "    #Apply Transformation Matrix to guide student within a specified epsilon-delta of the approximated saddle region\n",
    "    def applyTransformationMatrix(self, param_group):\n",
    "        #Calculate average norm between approx. saddle region and current parameters\n",
    "        total_norm = 0.0\n",
    "        for param, saddle_point in zip(param_group[\"params\"], param_group[\"teacher_saddle_points\"]):\n",
    "            norm_diff = torch.abs(torch.linalg.norm(param) - torch.linalg.norm(saddle_point))\n",
    "            total_norm += norm_diff\n",
    "        #Find average norm\n",
    "        average_norm = total_norm/len(param_group[\"params\"])\n",
    "        print(\"AVERAGE NORM FROM APPROX. SADDLE REGION:\", average_norm)\n",
    "        #Iterate over all parameters, determine if TM is needed, apply accordingly\n",
    "        for (param_number, param), (saddle_number, best_saddle_point) in zip(enumerate(param_group[\"params\"]), enumerate(param_group[\"teacher_saddle_points\"])):\n",
    "            #If this is the first step, initialize all saddle point seeking behaviours for the parameter to True\n",
    "            if self.state[\"STEP\"] == 1: \n",
    "                self.state[\"ALLOW_TEACHER_SADDLE_STEPS\"][param_number] = True\n",
    "            #Find Euclidean Distance Matrix between current parameters and saddle point\n",
    "            #If matrices are one dimensional, unsqueeze \n",
    "            #Only check DIAGONAL entries as opposed to all (diagonal entries should be identical if matrices are identical)\n",
    "            if len(param.shape) < 2: euclidean_distance = torch.diagonal(torch.cdist(param.unsqueeze(1), best_saddle_point.unsqueeze(1)))\n",
    "            #If the matrix is exactly two-dimensional, simply take the diagonal without unsqueezing\n",
    "            elif len(param.shape) == 2: euclidean_distance = torch.diagonal(torch.cdist(param, best_saddle_point))\n",
    "            #For matrices higher than 2 dimensions\n",
    "            #Take the diagonal of the diagonal matrix as this is a multidimensional tensor (the desired values are diag^2)\n",
    "            else: euclidean_distance = torch.diagonal(torch.diagonal(torch.cdist(param, best_saddle_point)))\n",
    "            #Check if all elements are below epsilon - first label the matrix by True or False depending on if they meet this condition\n",
    "            #False - below epsilon. True - above epsilon.\n",
    "            euclidean_distance_bool = euclidean_distance > param_group[\"epsilon\"]\n",
    "            #If any 'True' elements are present, continue taking steps towards the teacher saddle point region\n",
    "            if torch.any(euclidean_distance_bool).item() and self.state[\"ALLOW_TEACHER_SADDLE_STEPS\"][param_number]: \n",
    "                #Build elementwise transformation matrix between current parameters and saddle point parameters\n",
    "                #Divide matrices\n",
    "                transform_ewise_mat = torch.div(best_saddle_point, param)\n",
    "                #Multiply by current parameter matrix \n",
    "                #Add exponentially decaying weight to prevent long-term convergence hindering\n",
    "                weight = -2**(-self.state[\"STEP\"]/10)/2 + 1\n",
    "                #Apply TM * weight to transformation matrix\n",
    "                param.data = param.squeeze().mul(weight * transform_ewise_mat)\n",
    "                print('\\nTRANSFORMATION [{}] STEP TAKEN.'.format(param_number))\n",
    "                print('\\nTRANSFORMATION [{}] MATRIX NORM: {}'.format(param_number, torch.linalg.norm(transform_ewise_mat)))\n",
    "                print('\\nEUCLIDEAN DIST. [{}] MATRIX: {}'.format(param_number, euclidean_distance))\n",
    "                print('\\nEUCLIDEAN DIST. [{}] MATRIX NORM: {}'.format(param_number, torch.linalg.norm(euclidean_distance)))\n",
    "            #Otherwise, if this parameter is in epsilon range of the saddle point, disable taking targeted steps to this region for said parameter\n",
    "            else: self.state[\"ALLOW_TEACHER_SADDLE_STEPS\"][param_number] = False\n",
    "    \n",
    "    #Take Negative Hessian Eigensteps for a given step delta after the student is sufficiently close to the approximated saddle point region\n",
    "    def negativeHessianEigensteps(self, model, n_eigs, param_group):\n",
    "        #Check if all parameters are sufficiently close to the approx. saddle region and if within the step delta\n",
    "        #End if not true\n",
    "        if True in self.state[\"ALLOW_TEACHER_SADDLE_STEPS\"].values() or self.state[\"N_HESSIAN_NEG_EIGENSTEPS\"] > param_group[\"max_hessian_neg_eigensteps\"]: return\n",
    "        #Otherwise, begin taking negative Hessian eigenstep\n",
    "        #Iterate and save gradients (will otherwise be distrupted by Hessian eigenvalue computations)\n",
    "        saved_grads = {}\n",
    "        for param_n, param in enumerate(param_group[\"params\"]): saved_grads[param_n] = param.grad.clone().detach()\n",
    "        #Calculate top 2 Hessian eigenvalues and eigenvectors\n",
    "        hess_comp = hessian(model = model.model, criterion = model.loss_func, data = next(iter(model.dls.train)), cuda = True)\n",
    "        top_eigenvalues, top_eigenvectors = hess_comp.eigenvalues(top_n = n_eigs)\n",
    "        #Find position of smallest eigenvalue (largest negative eigenvalue) and the associated eigenvector\n",
    "        ev_index = top_eigenvalues.index(min(top_eigenvalues))\n",
    "        largest_negative_eigenvalue = torch.tensor(top_eigenvalues[ev_index])\n",
    "        largest_negative_eigenvector = top_eigenvectors[ev_index]\n",
    "        #Iterate over each parameter and multiply (rescale) the gradients by the eigenvector corresponding to the largest negative direction (smallest eigenvalue)\n",
    "        for (param_n, param), (eigenvector_n, eigenvector)in zip(enumerate(param_group[\"params\"]), enumerate(largest_negative_eigenvector)):\n",
    "            #Convert to PyTorch tensor\n",
    "            eigenvector = torch.tensor(eigenvector)\n",
    "            #Break if the eigenvalue is negative or 0\n",
    "            if largest_negative_eigenvalue >= 0: break\n",
    "            #Skip if the parameter has no gradient\n",
    "            if saved_grads[param_n] == None: continue\n",
    "            #Otherwise, make a copy of the gradient and reshape it to a vector\n",
    "            vec_grad = saved_grads[param_n]\n",
    "            #Use the formula p_t - [g(x)^T*v]*v to transform the parameters along the respective eigendirection (where v is the eigenvector)\n",
    "            #\n",
    "            neg_eigenstep = vec_grad.mul(eigenvector).mul(eigenvector)\n",
    "            #Reshape the eigenstep vector back into the size of the original matrix\n",
    "            #neg_eigenstep = torch.reshape(neg_eigenstep, param.grad.shape)\n",
    "            #Update the parameters by the learning rate * the negative eigenstep\n",
    "            #Compute weight\n",
    "            weight = (1/torch.sqrt(abs(largest_negative_eigenvalue))) * (2**(-(1/(param_group[\"max_hessian_neg_eigensteps\"]/1.5)) * self.state[\"STEP\"]))\n",
    "            param.data = param.data - (1 * neg_eigenstep)\n",
    "        #Increase counter of Hessian eigensteps taken.\n",
    "        self.state[\"N_HESSIAN_NEG_EIGENSTEPS\"] += 1\n",
    "        print(\"TAKEN NEGATIVE EIGENSTEP. TOP EIGENVALUES:\", top_eigenvalues)\n",
    "    \n",
    "    #Perturbed Gradient Descent/Negative Hessian Eigensteps for efficient saddle point escaping\n",
    "    def perturbedGD(self, model, current_loss, param_group):\n",
    "        #As per Perturbated Gradient Descent (PGD), add a small value to the current parameters if this is the case\n",
    "        for param_idx, param in enumerate(param_group[\"params\"]):\n",
    "            #Compare norm to gradient threshold\n",
    "            #Also check if the average norm between the aproximated saddle region and the model's current location in parameter space exceeds the PGD delta\n",
    "            #Determine whether SPRKD is within the PGD epoch limit\n",
    "            if torch.linalg.norm(param.grad) < self.state[\"GRADIENT_THRESHOLD\"] and avg_norm > param_group[\"PGD_delta\"] \\\n",
    "                and self.state[\"STEP\"]/len(model.dls.train) < param_group[\"PGD_epoch_limit\"]:\n",
    "                #If larger, add the perturbation\n",
    "                #Do so only if the cooldown steps have been fulfilled (at least 50)\n",
    "                #And, check that the saddle point region has already been reached (avoiding interference with transformation matrix computations)\n",
    "                if self.state[\"COOLDOWN_STEPS\"] == 0 and True not in self.state[\"ALLOW_TEACHER_SADDLE_STEPS\"].values():\n",
    "                    #If this is the first iteration or the loss trheshold has been met, simply perturb, save loss, and save parameters \n",
    "                    if self.state[\"STORED_LOSS\"] == 0.0 or self.state[\"STORED_LOSS\"] - current_loss >= 0.002: \n",
    "                        #Save current parameter to parameter history\n",
    "                        self.state[\"PARAM_HISTORY_PGD\"][param_idx] = param.clone().detach()\n",
    "                        #Perturbate parameters - add Gausian noise with variance 0.1\n",
    "                        param.data = param + ((math.sqrt(0.1)) * torch.rand_like(param))\n",
    "                        #Set loss\n",
    "                        self.state[\"STORED_LOSS\"] = current_loss\n",
    "                        print(\"Perturbed to avoid saddle point stagnation.\")\n",
    "                    #If the condition has not been met, revert to the point before the most recent perturbation\n",
    "                    elif self.state[\"STORED_LOSS\"] - current_loss < 0.002:\n",
    "                        param.data = self.state[\"PARAM_HISTORY_PGD\"][param_idx] = param.clone().detach()\n",
    "                        print(\"Perturbation ineffective [Reduction: {}]. Reverted to non-perturbed point.\".format(self.state[\"STORED_LOSS\"] - current_loss))\n",
    "                    self.state[\"COOLDOWN_STEPS\"] = param_group[\"cooldown_steps\"]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
